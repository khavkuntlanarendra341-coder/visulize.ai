# ğŸ§  Visualize.AI

> An AI-powered multimodal learning assistant that understands real-world images and teaches users how things work, how to fix them, and how to improve them through interactive visual reasoning.

![Visualize.AI Banner](https://via.placeholder.com/1200x400/0ea5e9/ffffff?text=Visualize.AI+-+Learn+How+Things+Work)

## ğŸŒŸ Features

### ğŸ–¼ï¸ Image Understanding
Upload any image of a machine, device, or system. The AI analyzes and identifies all visible components in context.

### ğŸ‘† Tap-to-Explain
Click on any part of the image to get focused, detailed explanations about that specific component.

### ğŸ’¡ What-If Mode
Explore hypothetical scenarios: "What if this component fails?" or "What happens if I skip this step?"

### ğŸ“Š Difficulty Slider
Adaptive explanations from Novice to Expert - the same image explained at your level.

### ğŸ§  Context Memory
Ask follow-up questions without re-uploading. The AI remembers your conversation context.

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| **AI Model** | Gemini 1.5 Flash Multimodal API |
| **Frontend** | React 18 + Tailwind CSS |
| **Image Interaction** | HTML Canvas / SVG Overlays |
| **Backend** | Node.js + Express |
| **State Management** | In-memory Session Store (Redis-ready) |
| **Hosting** | Vercel (Frontend) + Render/Railway (Backend) |

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- A Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/visualize-ai.git
cd visualize-ai
```

### 2. Setup Backend

```bash
cd backend
npm install

# Create .env file
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY

# Start the server
npm run dev
```

### 3. Setup Frontend

```bash
cd frontend
npm install

# Create .env file (optional - defaults to localhost:3001)
cp .env.example .env

# Start the development server
npm start
```

### 4. Open Your Browser

Navigate to `http://localhost:3000` and start exploring!

## ğŸ“ Project Structure

```
visualize.ai/
â”œâ”€â”€ frontend/                 # React Frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.js         # App header
â”‚   â”‚   â”‚   â”œâ”€â”€ ImageUploader.js  # Drag & drop upload
â”‚   â”‚   â”‚   â”œâ”€â”€ ImageCanvas.js    # Interactive canvas
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPanel.js      # AI chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ DifficultySlider.js
â”‚   â”‚   â”‚   â””â”€â”€ WhatIfMode.js
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js           # Backend API calls
â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ backend/                  # Express Backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.js       # Image upload & analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ ask.js           # Follow-up questions
â”‚   â”‚   â”‚   â””â”€â”€ session.js       # Session management
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.js        # Gemini API integration
â”‚   â”‚   â”‚   â””â”€â”€ sessionStore.js  # In-memory session store
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â””â”€â”€ errorHandler.js
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”‚
â””â”€â”€ README.md
```

## ğŸ¯ Use Cases

| Use Case | Description |
|----------|-------------|
| ğŸ“š **Visual Learning** | Understand how real-world objects work by tapping on parts |
| ğŸ”§ **DIY & Repair** | Diagnose issues and learn how to fix appliances visually |
| ğŸ“ **Skill Training** | Interactive onboarding for students, technicians, engineers |
| ğŸ“– **Education** | Multi-level explanations from the same real-world image |
| â™¿ **Accessibility** | Learn complex systems without manuals |
| ğŸ¤” **Problem Solving** | Ask "What if?" questions to explore cause-and-effect |

## ğŸ”§ API Endpoints

### POST `/api/analyze`
Upload and analyze an image.

**Request:** `multipart/form-data`
- `image`: Image file (JPEG, PNG, WebP, GIF)
- `difficulty`: String (Novice, Beginner, Intermediate, Advanced, Expert)

**Response:**
```json
{
  "sessionId": "uuid",
  "analysis": "AI analysis text...",
  "components": [{"name": "Component", "x": 50, "y": 30}]
}
```

### POST `/api/ask`
Ask a follow-up question.

**Request:**
```json
{
  "sessionId": "uuid",
  "question": "How does this work?",
  "tapPoint": {"x": 50, "y": 30},
  "difficulty": "Beginner"
}
```

### POST `/api/what-if`
Ask a hypothetical question.

**Request:**
```json
{
  "sessionId": "uuid",
  "scenario": "What if this component fails?",
  "difficulty": "Beginner"
}
```

## ğŸš¢ Deployment

### Frontend (Vercel)

```bash
cd frontend
npm run build
# Deploy to Vercel
vercel --prod
```

### Backend (Render/Railway)

1. Create a new Web Service
2. Connect your GitHub repository
3. Set build command: `npm install`
4. Set start command: `npm start`
5. Add environment variables:
   - `GEMINI_API_KEY`
   - `FRONTEND_URL`
   - `NODE_ENV=production`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ† Hackathon Submission

Built for [Hackathon Name] - an AI-powered multimodal learning assistant that transforms how people understand and interact with the physical world.

---

<p align="center">
  Made with â¤ï¸ and AI
  <br>
  <a href="https://devpost.com">View on Devpost</a>
</p>
